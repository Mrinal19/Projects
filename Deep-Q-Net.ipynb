{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (4250441579.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    def init (self, state_size, action_size):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# Define the network architecture \n",
    "class QNetwork(nn.Module):\n",
    "def init (self, state_size, action_size): \n",
    "    super(QNetwork, self). init () self.fc1 = \n",
    "    nn.Linear(state_size, 64) self.fc2 = \n",
    "    nn.Linear(64, 64)\n",
    "self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "def forward(self, x):\n",
    "x = torch.relu(self.fc1(x)) \n",
    "x = torch.relu(self.fc2(x)) \n",
    "x = self.fc3(x)\n",
    "return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the replay buffer class ReplayBuffer:\n",
    "def init (self, capacity): self.capacity = capacity self.buffer = [] self.index = 0\n",
    "\n",
    "def push(self, state, action, reward, next_state, done): if len(self.buffer) < self.capacity:\n",
    "self.buffer.append(None)\n",
    "self.buffer[self.index] = (state, action, reward, next_state, done) self.index = (self.index + 1) % self.capacity\n",
    "\n",
    "def sample(self, batch_size):\n",
    "batch = np.random.choice(len(self.buffer), batch_size, replace=False) states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "for i in batch:\n",
    "state, action, reward, next_state, done = self.buffer[i] states.append(state)\n",
    "actions.append(action) rewards.append(reward) next_states.append(next_state)\n",
    " \n",
    "dones.append(done) \n",
    "return (torch.tensor(np.array(states)).float(), torch.tensor(np.array(actions)).long(), torch.tensor(np.array(rewards)).unsqueeze(1).float(),\n",
    "        torch.tensor(np.array(next_states)).float(), torch.tensor(np.array(dones)).unsqueeze(1).int())\n",
    "\n",
    "def len  (self):\n",
    "return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Double DQN agent class DDQNAgent:\n",
    "def init (self, state_size, action_size, seed, learning_rate=1e-3, capacity=1000000,\n",
    "discount_factor=0.99, tau=1e-3, update_every=4, batch_size=64): self.state_size = state_size\n",
    "self.action_size = action_size self.seed = seed self.learning_rate = learning_rate\n",
    "self.discount_factor = discount_factor self.tau = tau\n",
    "self.update_every = update_every self.batch_size = batch_size self.steps = 0\n",
    "\n",
    "self.qnetwork_local = QNetwork(state_size, action_size) self.qnetwork_target = QNetwork(state_size, action_size) self.optimizer = optim.Adam(self.qnetwork_local.parameters(),\n",
    "lr=learning_rate)\n",
    "self.replay_buffer = ReplayBuffer(capacity) self.update_target_network()\n",
    "\n",
    "def step(self, state, action, reward, next_state, done): # Save experience in replay buffer\n",
    "self.replay_buffer.push(state, action, reward, next_state, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn every update_every steps self.steps += 1\n",
    "if self.steps % self.update_every == 0:\n",
    "if len(self.replay_buffer) > self.batch_size:\n",
    "experiences = self.replay_buffer.sample(self.batch_size) self.learn(experiences)\n",
    " \n",
    "\n",
    "def act(self, state, eps=0.0):\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") state = torch.from_numpy(state).float().unsqueeze(0).to(device) self.qnetwork_local.eval()\n",
    "with torch.no_grad():\n",
    "action_values = self.qnetwork_local(state) self.qnetwork_local.train()\n",
    "\n",
    "# Epsilon-greedy action selection if random.random() > eps:\n",
    "return np.argmax(action_values.cpu().data.numpy()) else:\n",
    "return random.choice(np.arange(self.action_size))\n",
    "\n",
    "def learn(self, experiences):\n",
    "states, actions, rewards, next_states, dones = experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max predicted Q values (for next states) from target model Q_targets_next =\n",
    "self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1) # Compute Q targets for current states\n",
    "Q_targets = rewards + self.discount_factor * (Q_targets_next * (1 -\n",
    "dones))\n",
    "\n",
    "# Get expected Q values from local model\n",
    "Q_expected = self.qnetwork_local(states).gather(1, actions.view(-1, 1))\n",
    "\n",
    "# Compute loss\n",
    "loss = F.mse_loss(Q_expected, Q_targets) # Minimize the loss self.optimizer.zero_grad() loss.backward()\n",
    "self.optimizer.step()   # Update target network self.soft_update(self.qnetwork_local, self.qnetwork_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_network(self):\n",
    "# Update target network parameters with polyak averaging\n",
    "for target_param, local_param in zip(self.qnetwork_target.parameters(), self.qnetwork_local.parameters()):\n",
    "target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
